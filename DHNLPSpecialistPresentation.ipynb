{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can Natural Language Processing (NLP) reframe scholarship across disciplines?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I started programming, it was to make tools that made my life as a humanist easier. For this talk, I wanted to put together a tool that I had come up with about five years ago, while studying abroad at St. Andrews. Then the intro to computer science class had me stumped, but now, with the new skills in programming that I have fostered for the past two and a half years, I decided to got back and try to make my idea a reality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I'll be showing: a search engine for a specific text\n",
    "* It will two inputs:\n",
    "    1. the text, split into sentences\n",
    "    2. a search term\n",
    "* It will give the user back a list of relevent sentences based on similarity with that search term\n",
    "\n",
    "This system allows the humanist to pick out important yet hidden sections of text, instead of agonizing over dogeared pages and trying to remember where a certain line was written. \n",
    "\n",
    "I wanted to focus on this tool because I think that walking you through how I built it will give you a good overview of how NLP methods can encourage interdisciplinary research. But I want to give you a little demo so that you understand the goal and know what to expect as we go on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search import searchEngine\n",
    "import spacy\n",
    "import pandas as pd\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "df = pd.read_csv('ushh_sentsrand200.csv')\n",
    "\n",
    "ushh_search = searchEngine(df=df, nlp=nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will talk at length about this dataset in a couple minutes, but to put it simply, it is made up of 200 random US congressional hearings. Because these hearings span a long time, we can query this search engine to discover what certain congresspeople said or asked about a variety of contemporary issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = 'family debt relief'\n",
    "search = ushh_search.search('serialized_data/rand200_output', search_term,context_size=3,title='title',date='date',speaker='speaker')\n",
    "ushh_search.displaySearch(search[0],search[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if you are a data scientist or interested in the implementation of this search engine, I have all of the code that I used in this notebook, as well as a module that you can call from the search.py file. All of the code for this presentation can be found at my GitHub: INCLUDE LINK!!!!\n",
    "\n",
    "Otherwise, if you are a humanist who is intersted in how to approach traditional questions in a novel way, I will show you how I thought about this problem, how I devised a solution to it and how you might address a research questions of your own in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all imports for the notebook\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import spacy\n",
    "import warnings\n",
    "from IPython.core.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import collections\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## United States Congressional Hearings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will show you methods to get and clean data and then how to prepare it for text processing. To create our search engine, the computer is going to need to turn all of the words of <br>a text into a special type of number called a vector. These vectors represent the semantic meaning of a word or sentence. We will be using spaCy's medium English model to generate these vectors, but before we can <br> we need to make sure our data is the correct form. \n",
    "\n",
    "From the Government Publishing Office: *A hearing is a meeting or session of a Senate, House, joint, or special committee of Congress, usually open to the public, to obtain information and opinions on <br> proposed legislation, conduct an investigation, or evaluate/oversee the activities of a government department or the implementation of a Federal law.*\n",
    "\n",
    "These documents cover a huge range of subjects from maritime law to medication regulation. Encoded in these documents are what our congresspeople think about certain issues. Thus, our search engine <br>will take in any word or phrase and return what opinions on that subject certain congresspeople have. In a certain sense, these views are available to anyone, but without NLP tools and methods they remain inaccessible <br> to most researchers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## archive.org mirror of the data\n",
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame('https://archive.org/details/us_house_hearings', width=1800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "I had to do a lot of data cleaning to get this dataset into a form that we could analyze. <br>\n",
    "If you are interested in webscraping or data structures, I would encourage you to read the following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeArchive(url):\n",
    "  r = requests.get(url)\n",
    "  r.encoding = 'utf-8'\n",
    "  soup = BeautifulSoup(r.text)\n",
    "\n",
    "  url_dict = {}\n",
    "  for div in soup.find_all('div', class_='C234'):\n",
    "    if isinstance(div.find('a'), type(None)):\n",
    "      continue\n",
    "    else:\n",
    "      download = div.find('a')['href'].replace('/details/', '/download/')\n",
    "      download = f'{download}/{download.split(\".\")[-1]}.htm'\n",
    "      url_dict[div.find('a').find('div', class_='ttl').text.strip()] = download\n",
    "  ushh = pd.DataFrame.from_dict(url_dict, orient='index').reset_index().rename(columns={'index':'title', 0:'url'})\n",
    "  return ushh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above strips all of the links to the raw textual data and the title of the hearing and deposits them into a dataframe, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(title, url):\n",
    "  r = requests.get(f'http://www.archive.org{url}')\n",
    "  r.encoding = 'utf-8'\n",
    "  soup = BeautifulSoup(r.text)\n",
    "  if not isinstance(soup.pre, type(None)):\n",
    "    text = soup.pre.text.replace('                                ','').replace('\\n','')\n",
    "    if title.upper() in text:\n",
    "      text = text[text.find(title.upper()):text.find('[Submissions for the record follow.]')].replace('TIFF','').replace('OMITTED','').replace('[GRAPHIC] [ ]', '')\n",
    "      return text \n",
    "    else:\n",
    "      return \"No data.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while this function can be applied to each of the links to retrieve the text and put it in the same dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFs = []\n",
    "for i in tqdm(range(133)):\n",
    "  url = f'https://archive.org/details/us_house_hearings?page={i+1}'\n",
    "  DFs.append(scrapeArchive(url))\n",
    "ushh = pd.concat(DFs).reset_index(drop=True)\n",
    "\n",
    "ushh['text'] = ushh.progress_apply(lambda x: getText(x['title'],x['url']), axis=1)\n",
    "ushh = ushh.drop(ushh.loc[ushh['text'] == 'No data.'].index).dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally, this method took 3 hours, 52 minutes and 24 seconds to complete, so below I will load in the completed dataset, serialized as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ushh = pd.read_csv('ushh827.csv').drop('Unnamed: 0', axis=1).dropna().reset_index(drop=True)\n",
    "ushh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have the text in a structured form. Now, we're done, right? .... Right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_html(image):\n",
    "     return f'<img src=\"{image}\" style=\"display:inline;margin:1px\"/>'\n",
    "files = ['ushh ex1.PNG','ushh ex2.PNG','ushh ex3.PNG']\n",
    "\n",
    "display(HTML('<p>Example of header</p>'))\n",
    "display(HTML(make_html(files[0])))\n",
    "display(HTML('<p>Example of unuseful pieces of data</p>'))\n",
    "display(HTML(make_html(files[1])))\n",
    "display(HTML('<p>Example of Example of how the transcript looks</p>'))\n",
    "display(HTML(make_html(files[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, our data wrangling has just begun. Now we need to:\n",
    "* Remove any documents that have no data\n",
    "* Clean the documents for NLP tools\n",
    "* Split up documents into speeches and then sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem: many documents have no substantive text that we can use for our tool\n",
    "## Solution: only full hearing transcripts have a date\n",
    "ushh['text_check'] = ushh['text'].apply(lambda x: True if re.search('[A-Z]+,\\s[A-Z]+\\s\\d{2},\\s\\d{4}|[A-Z]+,\\s[A-Z]+\\s\\d{1},\\s\\d{4}', x) else False) \n",
    "ushh = ushh.drop(ushh.loc[ushh['text_check'] == False].index)\n",
    "ushh = ushh.drop('text_check', axis=1)\n",
    "\n",
    "## Problem: all of the document have a long header that will skew our results\n",
    "## Solution: split the header off, on the date we checked for above \n",
    "def cleanEntry(text):\n",
    "    header = re.split('(?=([A-Z]+,\\s[A-Z]+\\s\\d{2},\\s\\d{4}))',text)[0]\n",
    "    clean_full = text[len(header):]\n",
    "    return header, clean_full\n",
    "ushh['clean_tup'] = ushh.text.apply(cleanEntry)\n",
    "ushh['header'] = ushh['clean_tup'].apply(lambda x: x[0])\n",
    "ushh['clean_full'] = ushh['clean_tup'].apply(lambda x: x[1])\n",
    "\n",
    "## Problem: we want to look at each sentence, but the document is split into speeches\n",
    "## Solution: each speech begins with the speaker's name\n",
    "ushh['clean_split'] = ushh.clean_full.apply(lambda x: re.split('(M[a-z]\\.\\s[A-Z][a-z]+\\.)|(Senator\\s[A-Z][a-z]+\\.)|(Dr\\.\\s[A-Z][a-z]+\\.)',x)[1:])\n",
    "ushh['date'] = ushh.clean_full.apply(lambda x: re.search('([A-Z]+,\\s[A-Z]+\\s\\d{2},\\s\\d{4})',x))\n",
    "ushh_speech_explode = ushh.explode('clean_split').reset_index()\n",
    "ushh_speech_explode['speech_check'] = ushh_speech_explode.clean_split.apply(lambda x: True if isinstance(x,str) else False)\n",
    "ushh_speech_explode = ushh_speech_explode.drop(ushh_speech_explode.loc[ushh_speech_explode['speech_check'] == False].index)\n",
    "\n",
    "ushh_speech_explode['speaker'] = ushh_speech_explode.clean_split.apply(lambda x: re.search('(M[a-z]\\.\\s[A-Z][a-z]+\\.)|(Senator\\s[A-Z][a-z]+\\.)|(Dr\\.\\s[A-Z][a-z]+\\.)',x))\n",
    "ushh_speech_explode = ushh_speech_explode.drop('speech_check',axis=1).reset_index(drop=True)\n",
    "\n",
    "## Problem: want to look at the sentence level, but the text is divided by speech\n",
    "## Solution nltk's sent_tokenize function\n",
    "ushh_speech_explode['sents'] = ushh_speech_explode.clean_split.apply(sent_tokenize)\n",
    "ushh_sents = ushh_speech_explode.explode('sents')\n",
    "\n",
    "## Further cleaning of textual artifacts\n",
    "ushh_sents = ushh_sents.drop(ushh_sents.loc[ushh_sents['sents'].str.contains('Whereupon')].index)\n",
    "ushh_sents['sents_check'] = ushh_sents['sents'].apply(lambda x: True if not x.isupper() else False)\n",
    "\n",
    "ushh_sents = ushh_sents.drop(ushh_sents.loc[ushh_sents['sents_check'] == False].index).drop(['sents_check','text','clean_tup','header','clean_full','url'],axis=1).reset_index(drop=True)\n",
    "ushh_sents['date'] = ushh_sents['date'].apply(lambda x: x.group(0) if not isinstance(x, type(None)) else 'Not Found')\n",
    "ushh_sents['speaker'] = ushh_sents['speaker'].apply(lambda x: x.group(0) if not isinstance(x, type(None)) else 'Not Found')\n",
    "\n",
    "ushh_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ushh = pd.read_csv('ushh827.csv')\n",
    "ushh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ushh_sents = pd.read_csv('ushh_sentsrand200.csv')\n",
    "ushh_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !spacy download en_core_web_md --quiet ## uncomment this line to download model, must do this the first time\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is spaCy doing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For right now, this spaCy language is a black box. We input unmarked text and we get back a spaCy doc object. <br>\n",
    "This special Python object has many features, but we are most interested in this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_word = nlp.vocab['word']\n",
    "a_word.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might look really complicated, but I'll show you below how we can intepret a word vector and all of the thing we can do with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode() \n",
    "\n",
    "words = ['dog','cat','king','queen','man','woman','red','blue']\n",
    "vectors = []\n",
    "for word in words:\n",
    "    vectors.append(nlp.vocab[word].vector)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(vectors)\n",
    "\n",
    "embeddings_df = pd.DataFrame({\"x\":embeddings_2d[:, 0], \"y\":embeddings_2d[:, 1], \"token\":words})\n",
    "\n",
    "fig = px.scatter(embeddings_df, x='x', y='y', opacity=0.5, hover_data=['token'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, similar words tend to stay together in this 2d representation of the word. <br>\n",
    "We can formalized this relation using cosine similarity. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, imagine that each point in that graph above has a line from the point to the origin, as we see below.<br>\n",
    "The angle between these two lines will then represent the difference between the two words, at least the difference as far as out model understands. <br>\n",
    "<img src='cosinesim.png'/>\n",
    "<br>\n",
    "We can then take the cosine of this angle to get a simiarlity score. <br>\n",
    "We choose to take the cosine because the cosine of 0, that is when the vectors are the same, is 1. <br>\n",
    "While, the cosine of 180, that is when the vectors are facing in opposite directions, is -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Designing a close reading tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ushh_sents['sent_doc'] = ushh_sents.sents.progress_apply(nlp)\n",
    "\n",
    "from spacy.tokens import DocBin\n",
    "bytes_file = open('serialized_data/rand200_output','rb').read()\n",
    "doc_bin = DocBin().from_bytes(bytes_file)\n",
    "ushh_sents['sent_doc'] =  pd.Series(doc_bin.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchWordOrPhraseUSSH(search_text, entries=5, context_size=2):\n",
    "  if ' ' in search_text:\n",
    "    ## if the search term is multi-token, we must pass it through our NLP pipeline\n",
    "    search_vec = nlp(search_text)\n",
    "  else:\n",
    "    ## if the search term is only one word, we can find the word vector directly\n",
    "    search_vec = nlp.vocab[search_text]\n",
    "  \n",
    "  ushh_search = ushh_sents\n",
    "  sim_score = ushh_search['sent_doc'].apply(lambda x: x.similarity(search_vec)).sort_values(ascending=False)[0:entries]\n",
    "  sim_df = sim_score.reset_index().rename(columns={'index':'org_idx'})\n",
    "\n",
    "  ## allows the user to choose how much of the context around the selected line they want to see\n",
    "  def createContext(org, context_size):\n",
    "    context = ushh_sents.sent_doc.iloc[org].text\n",
    "    for i in range(context_size):\n",
    "      if (i < len(ushh_sents)) and (i > 0):\n",
    "        context = ushh_sents.sent_doc.iloc[org-i].text + '\\n' + context\n",
    "        context = context + '\\n' + ushh_sents.sent_doc.iloc[org+i].text\n",
    "    return context\n",
    "\n",
    "  sim_df['context'] = sim_df['org_idx'].apply(lambda x: createContext(x, context_size))\n",
    "\n",
    "  ## interesting metadata we saved from the scraping phase\n",
    "  sim_df['title'] = sim_df['org_idx'].apply(lambda x: ushh_sents.title.iloc[x])\n",
    "  sim_df['date'] = sim_df['org_idx'].apply(lambda x: ushh_sents.date.iloc[x])\n",
    "  sim_df['speaker'] = sim_df['org_idx'].apply(lambda x: ushh_sents.speaker.iloc[x])\n",
    "  return sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = 'Global Warming' ## add search term!\n",
    "search = searchWordOrPhraseUSSH(term.lower().strip())\n",
    "\n",
    "display(HTML(f'<h2>{term}</h2>'))\n",
    "display(HTML('<br>'))\n",
    "for i in range(len(search)):\n",
    "  display(HTML(f'<small><i>{search.title.to_list()[i]}</i></small>'))\n",
    "  display(HTML(f'<small>{search.date.to_list()[i]}</small>'))\n",
    "  display(HTML(f'<small>{search.speaker.to_list()[i]}</small>'))\n",
    "  display(HTML(f'<small>Similarity Score: {round(search.sent_doc.to_list()[i], 3)}</small>'))\n",
    "  display(HTML(f'<p>{search.context.to_list()[i]}</p>'))\n",
    "  display(HTML('<br>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What next? Does this type of tool work for other fields?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Three Musketeers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Wikipedia: The Three Musketeers *is a French historical adventure novel written in 1844 by French author Alexandre Dumas. It is in the swashbuckler genre, which has heroic, chivalrous swordsmen who fight for justice. Set between 1625 and 1628, it recounts the adventures of a young man named d'Artagnan after he leaves home to travel to Paris, hoping to join the Musketeers of the Guard. Although d'Artagnan is not able to join this elite corps immediately, he is befriended by three of the most formidable musketeers of the age – Athos, Porthos and Aramis, \"the three musketeers\" or \"the three inseparables\" – and becomes involved in affairs of state and at court.*\n",
    "\n",
    "Like the US Congressional Hearings data we saw above, these texts are available, in fact enjoyable, to anyone, but they are not very accessible or navigable, especially for research and scholarship. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We have some web scraping and data cleaning to do, but much lass than before\n",
    "def scrapeText(url):\n",
    "  r = requests.get(url)\n",
    "  r.encoding = 'utf-8'\n",
    "  soup = BeautifulSoup(r.text)\n",
    "  \n",
    "  book = {}\n",
    "  for div in soup.find_all('div', class_='chapter'):\n",
    "    if div.h2 is not None:\n",
    "      title = div.h2.text\n",
    "      text = ''\n",
    "      for p in div.find_all('p'):\n",
    "        text += p.text\n",
    "      book[title] = (text.replace('\\n','').replace('\\r', ' '), url)\n",
    "  \n",
    "  book = pd.DataFrame.from_dict(book, orient='index').reset_index().rename(columns={'index':'title',0:'text'})\n",
    "  return book\n",
    "\n",
    "urls = [\n",
    "    'https://www.gutenberg.org/files/1257/1257-h/1257-h.htm', # The Three Musketeers\n",
    "    'https://www.gutenberg.org/files/1259/1259-h/1259-h.htm', # Twenty Years After\n",
    "    'https://www.gutenberg.org/files/2609/2609-h/2609-h.htm', # The Vicomte de Bragelonne\n",
    "    'https://www.gutenberg.org/files/2681/2681-h/2681-h.htm', # Ten Years Later\n",
    "    'https://www.gutenberg.org/files/2710/2710-h/2710-h.htm', # Louise de la Valliere\n",
    "    'https://www.gutenberg.org/files/2759/2759-h/2759-h.htm'  # The Man in the Iron Mask\n",
    "    ]\n",
    "\n",
    "DFs = []\n",
    "for i, url in enumerate(urls):\n",
    "  DFs.append(scrapeText(url))\n",
    "\n",
    "ttm = pd.concat(DFs)\n",
    "ttm = ttm.drop(ttm.loc[ttm['title'].str.contains('Footnotes')].index)\n",
    "ttm = ttm.drop(ttm.loc[ttm['text'] == ''].index).reset_index(drop=True)\n",
    "\n",
    "def url2name(url):\n",
    "  if url == 'https://www.gutenberg.org/files/1257/1257-h/1257-h.htm':\n",
    "    return 'The Three Musketeers'\n",
    "  elif url == 'https://www.gutenberg.org/files/1259/1259-h/1259-h.htm':\n",
    "    return 'Twenty Years After'\n",
    "  elif url == 'https://www.gutenberg.org/files/2609/2609-h/2609-h.htm':\n",
    "    return 'The Vicomte de Bragelonne'\n",
    "  elif url == 'https://www.gutenberg.org/files/2681/2681-h/2681-h.htm':\n",
    "    return 'Ten Years Later'\n",
    "  elif url == 'https://www.gutenberg.org/files/2710/2710-h/2710-h.htm':\n",
    "    return 'Louise de la Valliere'\n",
    "  else:\n",
    "    return 'The Man in the Iron Mask'\n",
    "  \n",
    "ttm['book'] = ttm[1].apply(url2name)\n",
    "ttm = ttm.drop(1, axis=1)\n",
    "ttm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sentence tokenization and application of the NLP method as above\n",
    "ttm['sents'] = ttm['text'].progress_apply(sent_tokenize)\n",
    "ttm_sents = ttm.explode('sents').reset_index(drop=True)\n",
    "#ttm_sents['sent_doc'] = ttm_sents['sents'].progress_apply(nlp)\n",
    "\n",
    "from spacy.tokens import DocBin\n",
    "bytes_file = open('serialized_data/ttm_output','rb').read()\n",
    "doc_bin = DocBin().from_bytes(bytes_file)\n",
    "ttm_sents['sent_doc'] =  pd.Series(doc_bin.get_docs(nlp.vocab))\n",
    "ttm_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What else can we do to our tool to make it more useful for this text in particular? Dumas writes for a primarily French audience, who are expected to have a bredth knowledge concerning western European geography. Using spaCy's off-the-shelf named entity recognition pipe, we can add a mapping component to lines which contain a geopolitical entities and locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## same basic code as before\n",
    "def searchWordOrPhraseTTM(search_text, entries=5, context_size=2):\n",
    "  if ' ' in search_text:\n",
    "    search_vec = nlp(search_text)\n",
    "  else:\n",
    "    search_vec = nlp.vocab[search_text]\n",
    "  \n",
    "  ttm_search = ttm_sents\n",
    "  sim_score = ttm_search['sent_doc'].apply(lambda x: x.similarity(search_vec)).sort_values(ascending=False)[0:entries]\n",
    "  sim_df = sim_score.reset_index().rename(columns={'index':'org_idx'})\n",
    "\n",
    "  def createContext(org, context_size):\n",
    "    context = ttm_sents.sent_doc.iloc[org].text\n",
    "    for i in range(context_size):\n",
    "      if (i < len(ttm_sents)) and (i > 0):\n",
    "        context = ttm_sents.sent_doc.iloc[org-i].text + '\\n' + context\n",
    "        context = context + '\\n' + ttm_sents.sent_doc.iloc[org+i].text\n",
    "    return context\n",
    "\n",
    "  ## returns a list of coordinates\n",
    "  def getLatLong(org, context_size):\n",
    "    for i in range(context_size):\n",
    "      ## default dicts are really useful for tasks like this\n",
    "      places = collections.defaultdict(int)\n",
    "      for ent in ttm_sents.sent_doc.iloc[org].ents:\n",
    "        ## using spaCy entity tags\n",
    "          places[ent.text] += 1\n",
    "      for ent in ttm_sents.sent_doc.iloc[org-i].ents:\n",
    "        if (ent.label_ == 'GPE') or (ent.label_ == 'LOC'):\n",
    "          places[ent.text] += 1\n",
    "      for ent in ttm_sents.sent_doc.iloc[org+i].ents:\n",
    "        if (ent.label_ == 'GPE') or (ent.label_ == 'LOC'):\n",
    "          places[ent.text] += 1\n",
    "      coords = []\n",
    "      # filtering out noise from spaCy NER\n",
    "      chars = ['Athos','Porthos','Aramis','Grimaud','Felton', 'Louise', 'Montalais', 'Mazarin']\n",
    "      for place in places:\n",
    "        if (place != 'one') and (place != 'four') and (place != 'first') and (place != 'Roman') and (place not in chars):\n",
    "          ## using geonames API to get coordinates\n",
    "          geoname = BeautifulSoup(requests.get(f'http://api.geonames.org/search?name_equals={place}&continentCode=EU&maxRows=10&username=pnadel').text).find('geoname')\n",
    "          if geoname != None:\n",
    "            lat = float(geoname.find('lat').get_text())\n",
    "            lng = float(geoname.find('lng').get_text())\n",
    "            coords.append((place,lat,lng,places[place]))\n",
    "    return coords\n",
    "\n",
    "  ## text \n",
    "  sim_df['context'] = sim_df['org_idx'].apply(lambda x: createContext(x, context_size))\n",
    "  \n",
    "  ## mapping data\n",
    "  sim_df['coords'] = sim_df['org_idx'].apply(lambda x: getLatLong(x, context_size))\n",
    "\n",
    "  ## useful metadata\n",
    "  sim_df['chapter'] = sim_df['org_idx'].apply(lambda x: ttm_sents.title.iloc[x])\n",
    "  sim_df['book'] = sim_df['org_idx'].apply(lambda x: ttm_sents.book.iloc[x])\n",
    "  return sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def europePlot(coords):\n",
    "  if len(coords) > 0:\n",
    "    ## coordinates in DF to allow for easier ploting\n",
    "    coord_df = pd.DataFrame(coords,columns=['place','lat','lng','c'])\n",
    "\n",
    "    ## open access geojson of Europe\n",
    "    filename = \"europe.geojson\"\n",
    "    file = open(filename)\n",
    "    df = gpd.read_file(file)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    ## select countries to plot\n",
    "    df[df['id'] == 'FR'].plot(ax=ax)\n",
    "    df[df['id'] == 'ES'].plot(ax=ax)\n",
    "    df[df['id'] == 'GB'].plot(ax=ax)\n",
    "    df[df['id'] == 'BE'].plot(ax=ax)\n",
    "    df[df['id'] == 'NL'].plot(ax=ax)\n",
    "    df[df['id'] == 'IE'].plot(ax=ax)\n",
    "    df[df['id'] == 'PT'].plot(ax=ax)\n",
    "    df[df['id'] == 'AD'].plot(ax=ax)\n",
    "    df[df['id'] == 'CH'].plot(ax=ax)\n",
    "\n",
    "    ## using pandas built-in plot function\n",
    "    coord_df.plot(x=\"lng\",y='lat',kind='scatter', c='c',colormap=\"Reds\", ax=ax)\n",
    "\n",
    "    ## making labels\n",
    "    for i in range(len(coord_df)):\n",
    "      plt.text(coord_df.iloc[i].lng,coord_df.iloc[i].lat,f'{coord_df.iloc[i].place}')\n",
    "      \n",
    "    ax.grid(b=True, alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = 'Chivalry' ## add search term!\n",
    "ttm_search = searchWordOrPhraseTTM(term.lower(),context_size=3)\n",
    "\n",
    "display(HTML(f'<h2>{term}</h2>'))\n",
    "display(HTML('<br>'))\n",
    "for i in range(len(ttm_search)):\n",
    "  display(HTML(f'<small><i>{ttm_search.book.to_list()[i]}</i></small>'))\n",
    "  display(HTML(f'<small>{ttm_search.chapter.to_list()[i]}</small>'))\n",
    "  display(HTML(f'<small>Similarity Score: {round(ttm_search.sent_doc.to_list()[i], 3)}</small>'))\n",
    "  display(HTML(f'<p>{ttm_search.context.to_list()[i]}</p>'))\n",
    "  europePlot(ttm_search.coords.to_list()[i])\n",
    "  display(HTML('<br>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the generalized method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to show off the generalized method by changing the language. We are now looking at the French (original) version of the Three Musketeers. All we will have to do to query this book instead of the English is change the language model from English to French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttm_fr = scrapeText('https://www.gutenberg.org/files/13951/13951-h/13951-h.htm')\n",
    "ttm_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttm_fr['sents'] = ttm_fr['text'].progress_apply(sent_tokenize)\n",
    "ttm_fr_sents = ttm_fr.explode('sents').reset_index(drop=True)\n",
    "ttm_fr_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get French model\n",
    "!spacy download fr_core_news_md --quiet\n",
    "nlp_fr = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search import searchEngine\n",
    "\n",
    "ttm_fr_search = searchEngine(\n",
    "    df = ttm_fr_sents,\n",
    "    nlp = nlp_fr\n",
    ")\n",
    "\n",
    "ttm_fr_search.spacyify('sents','ttm_fr_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = 'Chevalerie'\n",
    "search_fr = ttm_fr_search.search('serialized_data/ttm_fr_output', term, mapping=True, title='title')\n",
    "ttm_fr_search.displaySearch(search_fr[0],search_fr[1], mapping=True, title='title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaway: Close vs. distant reading\n",
    "In overview NLP methods through this example, we got to explore the power and limitations of both close reading and distant reading. <br>\n",
    "We saw how:\n",
    "* to process data and to clean a dataset for NLP\n",
    "* to apply an off-the-shelf language model to preprocessed data\n",
    "* to build a rudimentary user interface to interpret the model output\n",
    "* to generalize a method so that it can be used for many different texts\n",
    "\n",
    "#### What's next? \n",
    "The next step would be to deploy this search engine as an application. You have variety of options, like Streamlit, Voila or Django, but that is outside the purview of this talk, so hopefully I cna present on deployment at another time.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you for your attention!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3818054c1a980e27a6258be9fc914907b709c176106460b1f388431809e7839e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
